# Global LLM configuration

# google gemini
[llm]
# model = "gemini-2.0-flash" # The LLM model to use
model = "gemini-2.5-pro-exp-03-25"
base_url = "https://generativelanguage.googleapis.com/v1beta/openai" # API endpoint URL
api_key = "AIzaSyAWh01i9g1GmEi6LYoJb6H4NPpeUcZB7rE"                  # Your API key
max_tokens = 8192                                                    # Maximum number of tokens in the response
temperature = 0.0                                                    # Controls randomness


# openrouter
# [llm]
# model = "anthropic/claude-3.7-sonnet"                                                 # The LLM model to use
# base_url = "https://openrouter.ai/api/v1"                                             # API endpoint URL
# api_key = "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" # Your API key
# max_tokens = 8192                                                                     # Maximum number of tokens in the response
# temperature = 0.0                                                                     # Controls randomness

# [llm]https://generativelanguage.googleapis.com/v1beta/openai
# model = "claude-3-7-sonnet-20250219"        # The LLM model to use
# base_url = "https://api.anthropic.com/v1/"  # API endpoint URL
# api_key = "YOUR_API_KEY"                    # Your API key
# max_tokens = 8192                           # Maximum number of tokens in the response
# temperature = 0.0                           # Controls randomness

# [llm] # Amazon Bedrock
# api_type = "aws"                                       # Required
# model = "us.anthropic.claude-3-7-sonnet-20250219-v1:0" # Bedrock supported modelID
# base_url = "bedrock-runtime.us-west-2.amazonaws.com"   # Not used now
# max_tokens = 8192
# temperature = 1.0
# api_key = "bear"                                       # Required but not used for Bedrock

# [llm] #AZURE OPENAI:
# api_type= 'azure'
# model = "YOUR_MODEL_NAME" #"gpt-4o-mini"
# base_url = "{YOUR_AZURE_ENDPOINT.rstrip('/')}/openai/deployments/{AZURE_DEPOLYMENT_ID}"
# api_key = "AZURE API KEY"
# max_tokens = 8096
# temperature = 0.0
# api_version="AZURE API VERSION" #"2024-08-01-preview"

# [llm] #OLLAMA:
# api_type = 'ollama'
# model = "llama3.2"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# max_tokens = 4096
# temperature = 0.0

# Optional configuration for specific LLM models
[llm.vision]
# model = "gemini-2.0-flash" # The LLM model to use
model = "gemini-2.5-pro-exp-03-25"                                   # The LLM model to use
base_url = "https://generativelanguage.googleapis.com/v1beta/openai" # API endpoint URL
api_key = "AIzaSyAWh01i9g1GmEi6LYoJb6H4NPpeUcZB7rE"                  # Your API key
max_tokens = 8192                                                    # Maximum number of tokens in the response
temperature = 0.0                                                    # Controls randomness

# [llm.vision] #OLLAMA VISION:
# api_type = 'ollama'
# model = "llama3.2-vision"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# max_tokens = 4096
# temperature = 0.0


# Optional configuration for specific browser configuration
[browser]
# Whether to run browser in headless mode (default: false)
headless = false
# Disable browser security features (default: true)
# disable_security = true
# Extra arguments to pass to the browser
# extra_chromium_args = ["--no-sandbox", "--disable-gpu"]

# Path to a Chrome instance to use to connect to your normal browser
# e.g. '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'
#chrome_instance_path = ""
# Connect to a browser instance via WebSocket
#wss_url = ""
# Connect to a browser instance via CDP
#cdp_url = ""

# Optional configuration, Proxy settings for the browser
# [browser.proxy]
# server = "http://proxy-server:port"
# username = "proxy-username"
# password = "proxy-password"

# Optional configuration, Search settings.
# [search]
# Search engine for agent to use. Default is "Google", can be set to "Baidu" or "DuckDuckGo".
#engine = "Google"
# Fallback engine order. Default is ["DuckDuckGo", "Baidu"] - will try in this order after primary engine fails.
#fallback_engines = ["DuckDuckGo", "Baidu"]
# Seconds to wait before retrying all engines again when they all fail due to rate limits. Default is 60.
#retry_delay = 60
# Maximum number of times to retry all engines when all fail. Default is 3.
#max_retries = 3


## Sandbox configuration
#[sandbox]
#use_sandbox = false
#image = "python:3.12-slim"
#work_dir = "/workspace"
#memory_limit = "1g"  # 512m
#cpu_limit = 2.0
#timeout = 300
#network_enabled = true
